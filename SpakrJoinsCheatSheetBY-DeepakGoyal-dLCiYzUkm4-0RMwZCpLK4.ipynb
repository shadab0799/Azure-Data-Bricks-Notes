{"cells":[{"cell_type":"markdown","source":["#Spark Join Cheatsheet By Deepak Goyal (Azurelib Academy)\nCourse link: https://adeus.azurelib.com <br>\nMail at: admin@azurelib.com"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51637273-2a25-4b5c-9fed-fa4c7a9a1280","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\n# create two dataframes\ndf1 = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Jim\", 35)], \n                           [\"id\", \"name\", \"age\"])\ndf2 = spark.createDataFrame([(1, \"NYC\"), (2, \"LA\"), (3, \"DC\")], \n                           [\"id\", \"city\"])\n\n# perform the inner join\njoin_df = df1.join(df2, df1.id == df2.id, \"inner\")\n\n# display the joined dataframe\njoin_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb3cc925-85e3-4a1c-a05d-23c295e5f7e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n|  3| Jim| 35|  3|  DC|\n+---+----+---+---+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n|  3| Jim| 35|  3|  DC|\n+---+----+---+---+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#left join\njoin_df = df1.join(df2, df1.id == df2.id, \"left\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bf5dfcb-d819-404f-ae6d-8721d630af53","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#right join\njoin_df = df1.join(df2, df1.id == df2.id, \"right\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e93242a5-ffa9-4fbd-83a2-90dc27234002","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Broadcast join"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb65e9e9-9824-476e-8753-f62d45b84a73","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import broadcast\n\n\n# create two dataframes\ndf1 = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Jim\", 35)], \n                           [\"id\", \"name\", \"age\"])\ndf2 = spark.createDataFrame([(1, \"NYC\"), (2, \"LA\")], \n                           [\"id\", \"city\"])\n\n# broadcast the smaller dataframe\njoin_df = df1.join(broadcast(df2), df1.id == df2.id)\n\n# display the joined dataframe\njoin_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c0c7bf5-cd08-45ad-be42-e5aa96a8a50b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Join opitimization Partition based on id"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0f2a9c0-a756-4027-b60d-aab930a09cbb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Join opitimization Partition based on id\n\nfrom pyspark.sql import SparkSession\n\n\n\n# create two dataframes\ndf1 = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Jim\", 35)], \n                           [\"id\", \"name\", \"age\"])\ndf2 = spark.createDataFrame([(1, \"NYC\"), (2, \"LA\")], \n                           [\"id\", \"city\"])\n\n# repartition the dataframes on the join key\ndf1 = df1.repartition(df1.id)\ndf2 = df2.repartition(df2.id)\n\n# perform the join\njoin_df = df1.join(df2, df1.id == df2.id)\n\n# display the joined dataframe\njoin_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"875359a0-ab6a-497d-844e-713f193ddf35","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#SortMerge Join\nIn this example, we first create two dataframes df1 and df2, and then sort each dataframe on the join key id using the sortWithinPartitions method. Finally, we perform the join using the join method and display the resulting joined dataframe.\n\nNote that the sortWithinPartitions method sorts the data within each partition, not the entire dataframe. The data in each partition is sorted in ascending order by default, but you can specify a different sort order by passing the ascending argument."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4a98769-6252-4c72-94ce-113c612ac7fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n\n# create two dataframes\ndf1 = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Jim\", 35)], \n                           [\"id\", \"name\", \"age\"])\ndf2 = spark.createDataFrame([(1, \"NYC\"), (2, \"LA\")], \n                           [\"id\", \"city\"])\n\n# sort the dataframes on the join key\ndf1 = df1.sortWithinPartitions(\"id\")\ndf2 = df2.sortWithinPartitions(\"id\")\n\n# perform the join\njoin_df = df1.join(df2, df1.id == df2.id)\n\n# display the joined dataframe\njoin_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8c6e6a4-afdd-4c37-8354-ed3c9b8a98df","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+---+---+----+\n| id|name|age| id|city|\n+---+----+---+---+----+\n|  1|John| 25|  1| NYC|\n|  2|Jane| 30|  2|  LA|\n+---+----+---+---+----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Bucketing concept"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0590d06e-ede3-4f26-8598-933af38d0e6d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\n\n# create a Spark session\nspark = SparkSession.builder.appName(\"BucketingExample\").getOrCreate()\n\n# create a dataframe\ndf = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Jim\", 35)], \n                           [\"id\", \"name\", \"age\"])\n\n# bucket the dataframe\n#df = df.repartition(1, expr(\"id\")).write.bucketBy(2, \"id\").sortBy(\"id\").mode(\"overwrite\").parquet(\"/tmp/bucketed_data\").saveAsTable(\"bucketExample\")\n\ndf.write.format(\"parquet\").bucketBy(2, \"id\").sortBy(\"id\").option(\"path\", \"/tmp/bucketed_data\").saveAsTable(\"bucketExample\")\n\n\n# read the bucked data\nbucketed_df = spark.read.parquet(\"/tmp/bucketed_data\")\n\n# display the bucked data\nbucketed_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1b7ad84-2304-49e0-93eb-f3f26c67dda1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|John| 25|\n|  2|Jane| 30|\n|  3| Jim| 35|\n+---+----+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|John| 25|\n|  2|Jane| 30|\n|  3| Jim| 35|\n+---+----+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nls /tmp/bucketed_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4bced05-1e1a-4919-8901-59b7a677cc70","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/tmp/bucketed_data/_SUCCESS","_SUCCESS",0,1675936832000],["dbfs:/tmp/bucketed_data/_committed_1731950025197263571","_committed_1731950025197263571",339,1675936832000],["dbfs:/tmp/bucketed_data/_started_1731950025197263571","_started_1731950025197263571",0,1675936828000],["dbfs:/tmp/bucketed_data/part-00002-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-86-1_00001.c000.snappy.parquet","part-00002-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-86-1_00001.c000.snappy.parquet",1080,1675936831000],["dbfs:/tmp/bucketed_data/part-00005-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-89-1_00000.c000.snappy.parquet","part-00005-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-89-1_00000.c000.snappy.parquet",1079,1675936831000],["dbfs:/tmp/bucketed_data/part-00007-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-91-1_00001.c000.snappy.parquet","part-00007-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-91-1_00001.c000.snappy.parquet",1073,1675936831000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/bucketed_data/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1675936832000</td></tr><tr><td>dbfs:/tmp/bucketed_data/_committed_1731950025197263571</td><td>_committed_1731950025197263571</td><td>339</td><td>1675936832000</td></tr><tr><td>dbfs:/tmp/bucketed_data/_started_1731950025197263571</td><td>_started_1731950025197263571</td><td>0</td><td>1675936828000</td></tr><tr><td>dbfs:/tmp/bucketed_data/part-00002-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-86-1_00001.c000.snappy.parquet</td><td>part-00002-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-86-1_00001.c000.snappy.parquet</td><td>1080</td><td>1675936831000</td></tr><tr><td>dbfs:/tmp/bucketed_data/part-00005-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-89-1_00000.c000.snappy.parquet</td><td>part-00005-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-89-1_00000.c000.snappy.parquet</td><td>1079</td><td>1675936831000</td></tr><tr><td>dbfs:/tmp/bucketed_data/part-00007-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-91-1_00001.c000.snappy.parquet</td><td>part-00007-tid-1731950025197263571-c59e3e92-734f-4eca-8d89-f15e89cfec6f-91-1_00001.c000.snappy.parquet</td><td>1073</td><td>1675936831000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Incremental Data load using the append mode"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7c7c2e0-e1c2-4589-a641-2a0c6d96a438","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Load new data into a dataframe\nnew_data_df = spark.read.parquet(\"/data/new_data\")\n\n# Append the new data to an existing Delta Lake table\nnew_data_df.write.format(\"delta\").mode(\"append\").save(\"/delta/events\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5c570e7-6478-4bfd-96b1-53f221de0dc5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Incremental Data load using the delta table Merge command"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"803aaefc-962c-429b-8573-55ec2db4ade8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Load new data into a dataframe\nnew_data_df = spark.read.parquet(\"/data/new_data\")\n\n# Create a temporary table for the new data\nnew_data_df.createOrReplaceTempView(\"new_data\")\n\n# Perform a MERGE statement to update the data in the storage system\nspark.sql(\"\"\"\n  MERGE INTO events \n  USING new_data \n  ON events.id = new_data.id \n  WHEN MATCHED THEN \n    UPDATE SET * \n  WHEN NOT MATCHED THEN \n    INSERT * \n\"\"\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4607af0a-3ae9-49f9-a4be-246f6f5e1132","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Map Join /Brodacast Join"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf30cd4a-4aac-4152-a343-f41a2aad72a8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n\n# Load the large table\nlarge_table = spark.read.parquet(\"/data/large_table\")\n\n# Load the small table\nsmall_table = spark.read.parquet(\"/data/small_table\")\n\n# Broadcast the small table\nbroadcast_small_table = broadcast(small_table)\n\n# Perform the join\nresult = large_table.join(broadcast_small_table, \"id\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92de4afd-300d-44f6-81d8-d9b63542ba87","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#UDF example"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38470ad0-c8e5-43e0-8696-8b81163e3669","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the UDF\ndef to_upper(s):\n    return s.upper()\n\n# Register the UDF\nto_upper_udf = udf(to_upper, StringType())\n\n# Create a DataFrame\ndf = spark.createDataFrame([(\"Hello\",), (\"World\",)], [\"text\"])\n\n# Use the UDF in Spark SQL\ndf.select(to_upper_udf(df[\"text\"]).alias(\"text_upper\")).show()\n\n# Output:\n# +---------+\n# |text_upper|\n# +---------+\n# |     HELLO|\n# |     WORLD|\n# +---------+\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32b13dce-2c17-46a6-adbd-1eb59bc29863","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Another example\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, FloatType, DecimalType\nfrom decimal import Decimal\n\n# Define the UDF\ndef string_float_to_decimal(string, float_value):\n    return Decimal(string) * Decimal(float_value)\n\n# Register the UDF\nstring_float_to_decimal_udf = udf(string_float_to_decimal, DecimalType(10, 2))\n\n# Create a DataFrame\ndf = spark.createDataFrame([(\"1\", 1.0), (\"2\", 2.0)], [\"string\", \"float_value\"])\n\n# Use the UDF in Spark SQL\ndf.select(string_float_to_decimal_udf(df[\"string\"], df[\"float_value\"]).alias(\"decimal_result\")).show()\n\n# Output:\n# +-------------+\n# |decimal_result|\n# +-------------+\n# |         1.00|\n# |         4.00|\n# +-------------+"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"445a8ced-1545-4e92-bbd5-087aa2d5c730","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\nbase = spark.range(16000000)\n\n#Write non-bucketed table\nbase.write.format(\"parquet\").saveAsTable(\"unbucketed2\")\n\n#// Write bucketed table\nbase.write.format(\"parquet\").bucketBy(16, \"id\").saveAsTable(\"bucketed2\")\n\nt1 = spark.table(\"unbucketed2\")\nt2 = spark.table(\"bucketed2\")\nt3 = spark.table(\"bucketed2\")\n\n#// Unbucketed - bucketed join. Both sides need to be repartitioned.\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b44a258-f7eb-4915-b271-0af352cee28d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3990509121387465>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#Write non-bucketed table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mbase\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"unbucketed2\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m#// Write bucketed table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msaveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1039\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mformat\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1041\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1042\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m     def json(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table `spark_catalog`.`default`.`unbucketed2` already exists.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table `spark_catalog`.`default`.`unbucketed2` already exists.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3990509121387465>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#Write non-bucketed table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mbase\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"unbucketed2\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m#// Write bucketed table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msaveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1039\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mformat\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1041\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1042\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m     def json(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table `spark_catalog`.`default`.`unbucketed2` already exists."]}}],"execution_count":0},{"cell_type":"code","source":["t1.join(t2, \"id\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74eccf99-f414-41bf-a8e0-252dc2b65bc5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#47L]\n   +- SortMergeJoin [id#47L], [id#49L], Inner\n      :- Sort [id#47L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id#47L, 16), ENSURE_REQUIREMENTS, [plan_id=129]\n      :     +- Filter isnotnull(id#47L)\n      :        +- FileScan parquet spark_catalog.default.unbucketed2[id#47L] Batched: true, DataFilters: [isnotnull(id#47L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/unbucketed2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\n      +- Sort [id#49L ASC NULLS FIRST], false, 0\n         +- Filter isnotnull(id#49L)\n            +- FileScan parquet spark_catalog.default.bucketed2[id#49L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#49L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/bucketed2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 16 out of 16\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#47L]\n   +- SortMergeJoin [id#47L], [id#49L], Inner\n      :- Sort [id#47L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id#47L, 16), ENSURE_REQUIREMENTS, [plan_id=129]\n      :     +- Filter isnotnull(id#47L)\n      :        +- FileScan parquet spark_catalog.default.unbucketed2[id#47L] Batched: true, DataFilters: [isnotnull(id#47L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/unbucketed2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\n      +- Sort [id#49L ASC NULLS FIRST], false, 0\n         +- Filter isnotnull(id#49L)\n            +- FileScan parquet spark_catalog.default.bucketed2[id#49L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#49L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/bucketed2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 16 out of 16\n\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Joins","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":509476065662596,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4409765008949619}},"nbformat":4,"nbformat_minor":0}
